{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 4. Mécanisme d’Attention\n",
        "\n",
        "Implémentation de l’attention scaled dot-product en NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x, axis=-1):\n",
        "    exp_x = np.exp(x - np.max(x, axis=axis, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=axis, keepdims=True)\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    d_k = Q.shape[-1]\n",
        "    scores = np.dot(Q, K.T) / np.sqrt(d_k)\n",
        "    if mask is not None:\n",
        "        scores = scores * mask + -1e9 * (1 - mask)\n",
        "    weights = softmax(scores, axis=-1)\n",
        "    return np.dot(weights, V)\n",
        "\n",
        "# Exemple\n",
        "np.random.seed(42)\n",
        "seq_len, d_k, d_v = 4, 8, 8\n",
        "Q = np.random.randn(seq_len, d_k)\n",
        "K = np.random.randn(seq_len, d_k)\n",
        "V = np.random.randn(seq_len, d_v)\n",
        "\n",
        "# Masque causal\n",
        "mask = np.tril(np.ones((seq_len, seq_len)))\n",
        "out = scaled_dot_product_attention(Q, K, V, mask=mask)\n",
        "print(\"Forme de la sortie :\", out.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
