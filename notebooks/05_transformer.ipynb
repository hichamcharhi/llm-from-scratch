{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 5. Architecture Transformer\n",
        "\n",
        "Bloc de décodeur complet en NumPy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class LayerNorm:\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        self.eps = eps\n",
        "        self.gamma = np.ones(dim)\n",
        "        self.beta = np.zeros(dim)\n",
        "    def __call__(self, x):\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        var = np.var(x, axis=-1, keepdims=True)\n",
        "        x_norm = (x - mean) / np.sqrt(var + self.eps)\n",
        "        return self.gamma * x_norm + self.beta\n",
        "\n",
        "class FeedForward:\n",
        "    def __init__(self, dim, hidden_dim):\n",
        "        self.w1 = np.random.randn(dim, hidden_dim) * 0.01\n",
        "        self.w2 = np.random.randn(hidden_dim, dim) * 0.01\n",
        "    def __call__(self, x):\n",
        "        return np.dot(np.maximum(0, np.dot(x, self.w1)), self.w2)\n",
        "\n",
        "class MultiHeadAttention:\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        self.n_head = n_head\n",
        "        self.head_dim = n_embd // n_head\n",
        "        self.c_attn = np.random.randn(n_embd, 3 * n_embd) * 0.01\n",
        "        self.c_proj = np.random.randn(n_embd, n_embd) * 0.01\n",
        "    def __call__(self, x):\n",
        "        B, T, C = x.shape\n",
        "        qkv = np.dot(x, self.c_attn)\n",
        "        q, k, v = np.split(qkv, 3, axis=-1)\n",
        "        q = q.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
        "        k = k.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
        "        v = v.reshape(B, T, self.n_head, self.head_dim).transpose(0, 2, 1, 3)\n",
        "        att = np.matmul(q, k.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
        "        mask = np.tril(np.ones((T, T)))\n",
        "        att = att - 1e9 * (1 - mask)\n",
        "        att = np.exp(att - np.max(att, axis=-1, keepdims=True))\n",
        "        att = att / np.sum(att, axis=-1, keepdims=True)\n",
        "        y = np.matmul(att, v)\n",
        "        y = y.transpose(0, 2, 1, 3).reshape(B, T, C)\n",
        "        return np.dot(y, self.c_proj)\n",
        "\n",
        "class TransformerBlock:\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        self.ln1 = LayerNorm(n_embd)\n",
        "        self.attn = MultiHeadAttention(n_embd, n_head)\n",
        "        self.ln2 = LayerNorm(n_embd)\n",
        "        self.mlp = FeedForward(n_embd, 4 * n_embd)\n",
        "    def __call__(self, x):\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.mlp(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# Test\n",
        "block = TransformerBlock(64, 4)\n",
        "x = np.random.randn(1, 16, 64)\n",
        "out = block(x)\n",
        "print(\"Forme après TransformerBlock :\", out.shape)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
